{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\ndef build_colorization_model(input_shape=(256, 256, 1)):\\n    \"\"\"\\n    Builds a colorization model using transfer learning with VGG19 and an encoder-decoder architecture.\\n    \"\"\"\\n    # Input layer for grayscale images\\n    inputs = Input(shape=input_shape)\\n\\n    # Convert grayscale input to 3 channels to fit VGG19 input requirements\\n    def grayscale_to_rgb(x):\\n        return tf.image.grayscale_to_rgb(x)\\n    \\n    rgb_inputs = Lambda(grayscale_to_rgb)(inputs)\\n\\n    # VGG19 feature extractor for guiding colorization (use pre-trained weights for transfer learning)\\n    vgg19_base = VGG19(weights=\\'imagenet\\', include_top=False, input_shape=(256, 256, 3))\\n    vgg19_features = Model(inputs=vgg19_base.input, outputs=vgg19_base.get_layer(\\'block2_conv2\\').output)\\n\\n    # Extract semantic features from RGB image using VGG19\\n    vgg_features = vgg19_features(rgb_inputs)\\n\\n    # Encoder\\n    conv1 = Conv2D(64, (3, 3), activation=\\'relu\\', padding=\\'same\\')(vgg_features)\\n    conv2 = Conv2D(128, (3, 3), activation=\\'relu\\', padding=\\'same\\')(conv1)\\n\\n    # Decoder\\n    up1 = UpSampling2D((2, 2))(conv2)\\n    conv3 = Conv2D(128, (3, 3), activation=\\'relu\\', padding=\\'same\\')(up1)\\n    up2 = UpSampling2D((2, 2))(conv3)\\n    conv4 = Conv2D(64, (3, 3), activation=\\'relu\\', padding=\\'same\\')(up2)\\n\\n    # Final output layer to produce 3-channel RGB image\\n    output = Conv2D(3, (3, 3), activation=\\'sigmoid\\', padding=\\'same\\')(conv4)\\n\\n    # Define the colorization model\\n    model = Model(inputs=inputs, outputs=output)\\n    return model\\n\\n\\n    \\n\\n\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Resizing\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, UpSampling2D, Concatenate, Lambda\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "\n",
    "# Function to build VGG19 for perceptual loss calculation\n",
    "def build_vgg19_perceptual_model():\n",
    "    \"\"\"\n",
    "    Load the VGG19 model pre-trained on ImageNet and create a model that outputs the feature maps\n",
    "    from specific layers used for perceptual loss calculation.\n",
    "    \"\"\"\n",
    "    vgg19 = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "\n",
    "    # Freeze all VGG19 layers\n",
    "    for layer in vgg19.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Select intermediate layers for perceptual loss (e.g., block4_conv4)\n",
    "    feature_extractor = Model(inputs=vgg19.input, outputs=vgg19.get_layer('block4_conv4').output)\n",
    "\n",
    "    return feature_extractor\n",
    "\n",
    "'''\n",
    "\n",
    "# Perceptual loss function using VGG19\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute perceptual loss between the ground truth and generated images.\n",
    "    \"\"\"\n",
    "    vgg = build_vgg19_perceptual_model()\n",
    "    y_true_features = vgg(y_true)\n",
    "    y_pred_features = vgg(y_pred)\n",
    "    loss = MeanSquaredError()(y_true_features, y_pred_features)\n",
    "    return loss\n",
    "\n",
    "'''\n",
    "\n",
    "# Build the colorization generator model with transfer learning\n",
    "\n",
    "def build_colorization_model(input_shape=(256, 256, 1)):\n",
    "    \"\"\"\n",
    "    Builds a colorization model using transfer learning with VGG19 and an encoder-decoder architecture.\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "    \n",
    "    def grayscale_to_rgb(x):\n",
    "        return tf.image.grayscale_to_rgb(x)\n",
    "    \n",
    "    rgb_inputs = Lambda(grayscale_to_rgb)(inputs)\n",
    "    \n",
    "    vgg19_base = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "    vgg19_features = Model(inputs=vgg19_base.input, outputs=vgg19_base.get_layer('block2_conv2').output)\n",
    "    \n",
    "    vgg_features = vgg19_features(rgb_inputs)\n",
    "    \n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(vgg_features)\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv1)\n",
    "    conv3 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv2)\n",
    "\n",
    "    # Decoder\n",
    "    up1 = UpSampling2D((2, 2))(conv3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(up1)\n",
    "    up2 = UpSampling2D((2, 2))(conv4)\n",
    "    conv5 = Conv2D(128, (3, 3), activation='relu', padding='same')(up2)\n",
    "    up3 = UpSampling2D((2, 2))(conv5)\n",
    "    conv6 = Conv2D(64, (3, 3), activation='relu', padding='same')(up3)\n",
    "\n",
    "    # Final output layer to produce 3-channel RGB image\n",
    "    output = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(conv6)\n",
    "    \n",
    "    # Ensure output size matches input size\n",
    "    output = Resizing(256, 256)(output)\n",
    "    \n",
    "    model = Model(inputs, output)\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "def build_colorization_model(input_shape=(256, 256, 1)):\n",
    "    \"\"\"\n",
    "    Builds a colorization model using transfer learning with VGG19 and an encoder-decoder architecture.\n",
    "    \"\"\"\n",
    "    # Input layer for grayscale images\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # Convert grayscale input to 3 channels to fit VGG19 input requirements\n",
    "    def grayscale_to_rgb(x):\n",
    "        return tf.image.grayscale_to_rgb(x)\n",
    "    \n",
    "    rgb_inputs = Lambda(grayscale_to_rgb)(inputs)\n",
    "\n",
    "    # VGG19 feature extractor for guiding colorization (use pre-trained weights for transfer learning)\n",
    "    vgg19_base = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "    vgg19_features = Model(inputs=vgg19_base.input, outputs=vgg19_base.get_layer('block2_conv2').output)\n",
    "\n",
    "    # Extract semantic features from RGB image using VGG19\n",
    "    vgg_features = vgg19_features(rgb_inputs)\n",
    "\n",
    "    # Encoder\n",
    "    conv1 = Conv2D(64, (3, 3), activation='relu', padding='same')(vgg_features)\n",
    "    conv2 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv1)\n",
    "\n",
    "    # Decoder\n",
    "    up1 = UpSampling2D((2, 2))(conv2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(up1)\n",
    "    up2 = UpSampling2D((2, 2))(conv3)\n",
    "    conv4 = Conv2D(64, (3, 3), activation='relu', padding='same')(up2)\n",
    "\n",
    "    # Final output layer to produce 3-channel RGB image\n",
    "    output = Conv2D(3, (3, 3), activation='sigmoid', padding='same')(conv4)\n",
    "\n",
    "    # Define the colorization model\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    return model\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# Define your image paths\n",
    "grayscale_images_path = r\"C:\\Users\\dhana\\Downloads\\SAR Orig\\v_2\\urban\\s1\"\n",
    "colorful_images_path = r\"C:\\Users\\dhana\\Downloads\\SAR Orig\\v_2\\urban\\s2\"\n",
    "\n",
    "def load_image(image_path, target_size=(256, 256), grayscale=False):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image.\n",
    "    \n",
    "    Args:\n",
    "    - image_path (str): Path to the image.\n",
    "    - target_size (tuple): Desired size of the image after resizing.\n",
    "    - grayscale (bool): Whether to load the image in grayscale mode.\n",
    "    \n",
    "    Returns:\n",
    "    - Preprocessed image tensor.\n",
    "    \"\"\"\n",
    "    image = tf.io.read_file(image_path)\n",
    "    \n",
    "    if grayscale:\n",
    "        image = tf.image.decode_image(image, channels=1)  # Load as grayscale\n",
    "    else:\n",
    "        image = tf.image.decode_image(image, channels=3)  # Load as RGB\n",
    "    \n",
    "    # Resize the image to the target size\n",
    "    image = tf.image.resize(image, target_size)\n",
    "    \n",
    "    # Normalize the image to [0, 1]\n",
    "    image = tf.cast(image, tf.float32) / 255.0\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_data(batch_size=16):\n",
    "    \"\"\"\n",
    "    Load and preprocess data from the specified directories.\n",
    "    \n",
    "    Args:\n",
    "    - batch_size (int): Number of images per batch.\n",
    "    \n",
    "    Returns:\n",
    "    - sar_images (Tensor): Batch of grayscale SAR images.\n",
    "    - optical_images (Tensor): Batch of corresponding optical images.\n",
    "    \"\"\"\n",
    "    # Get list of image file paths\n",
    "    sar_image_paths = sorted([os.path.join(grayscale_images_path, fname) for fname in os.listdir(grayscale_images_path)])\n",
    "    optical_image_paths = sorted([os.path.join(colorful_images_path, fname) for fname in os.listdir(colorful_images_path)])\n",
    "    \n",
    "    # Ensure matching number of SAR and optical images\n",
    "    assert len(sar_image_paths) == len(optical_image_paths), \"Mismatch between number of SAR and optical images.\"\n",
    "    \n",
    "    # Randomly select a batch of images\n",
    "    indices = tf.random.shuffle(tf.range(len(sar_image_paths)))[:batch_size]\n",
    "    \n",
    "    sar_images = [load_image(sar_image_paths[i], target_size=(256, 256), grayscale=True) for i in indices]\n",
    "    optical_images = [load_image(optical_image_paths[i], target_size=(256, 256), grayscale=False) for i in indices]\n",
    "    \n",
    "    # Stack images into a batch\n",
    "    sar_images = tf.stack(sar_images)\n",
    "    optical_images = tf.stack(optical_images)\n",
    "\n",
    "    optical_images = tf.image.resize(optical_images, (256, 256))\n",
    "    \n",
    "    return sar_images, optical_images\n",
    "\n",
    "def perceptual_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the perceptual loss using VGG19.\n",
    "    \n",
    "    Args:\n",
    "    - y_true (Tensor): Ground truth images.\n",
    "    - y_pred (Tensor): Generated images by the model.\n",
    "    \n",
    "    Returns:\n",
    "    - loss (Tensor): Perceptual loss computed between VGG features of y_true and y_pred.\n",
    "    \"\"\"\n",
    "    vgg = VGG19(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "    vgg.trainable = False\n",
    "    \n",
    "    # Select intermediate layers for feature extraction\n",
    "    selected_layers = [vgg.get_layer('block3_conv3').output, vgg.get_layer('block4_conv4').output]\n",
    "    feature_extractor = Model(inputs=vgg.input, outputs=selected_layers)\n",
    "    \n",
    "    # Extract features from both ground truth and generated images\n",
    "    y_true_features = feature_extractor(y_true)\n",
    "    y_pred_features = feature_extractor(y_pred)\n",
    "    \n",
    "    # Compute the perceptual loss as the mean squared error between features\n",
    "    loss = K.mean(K.square(y_true_features[0] - y_pred_features[0])) + K.mean(K.square(y_true_features[1] - y_pred_features[1]))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "'''generator.compile(optimizer=Adam(0.0008, 0.5), loss=perceptual_loss)\n",
    "'''\n",
    "\n",
    "\n",
    "'''generator = build_colorization_model(input_shape=(256, 256, 1))\n",
    "\n",
    "generator.compile(optimizer=Adam(0.0011, 0.5), loss='mean_squared_error')  # Replace with perceptual_loss if defined\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "batch_size = 16\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sar_images, optical_images = load_data(batch_size)\n",
    "    \n",
    "    # Ensure input data size matches model input\n",
    "    sar_images = tf.image.resize(sar_images, (256, 256))\n",
    "    optical_images = tf.image.resize(optical_images, (256, 256))\n",
    "    \n",
    "    # Train the generator\n",
    "    loss = generator.train_on_batch(sar_images, optical_images)\n",
    "\n",
    "    # Print the progress\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "    # Save the model periodically\n",
    "    if ((epoch + 1) > 500) and ((epoch + 1) % 25 == 0):\n",
    "        generator.save(f\"08_09_2024_NEW_colorization_model_epoch_{epoch+1}.h5\")\n",
    "\n",
    "print(\"Training completed.\")'''\n",
    "\n",
    "\n",
    "\n",
    "# Build and compile the model\n",
    "generator = build_colorization_model(input_shape=(256, 256, 1))\n",
    "generator.compile(optimizer=Adam(0.0011, 0.5), loss='mean_squared_error')\n",
    "\n",
    "# Training loop\n",
    "epochs = 10000\n",
    "batch_size = 16\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sar_images, optical_images = load_data(batch_size)\n",
    "    \n",
    "    # Ensure input data size matches model input\n",
    "    sar_images = tf.image.resize(sar_images, (256, 256))\n",
    "    optical_images = tf.image.resize(optical_images, (256, 256))\n",
    "    \n",
    "    # Ensure sar_images have shape (batch_size, 256, 256, 1)\n",
    "    if sar_images.shape[-1] != 1:\n",
    "        sar_images = tf.image.rgb_to_grayscale(sar_images)\n",
    "    \n",
    "    # Train the generator\n",
    "    loss = generator.train_on_batch(sar_images, optical_images)\n",
    "\n",
    "    # Print the progress\n",
    "    print(f\"Epoch: {epoch+1}/{epochs}, Loss: {loss}\")\n",
    "\n",
    "    # Save the model periodically\n",
    "    if ((epoch + 1) > 500) and ((epoch + 1) % 25 == 0):\n",
    "        generator.save(f\"08_09_2024_NEW_colorization_model_epoch_{epoch+1}.h5\")\n",
    "\n",
    "print(\"Training completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "generator.save(f\"08_09_2024_NEW_colorization_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 1775-1776: truncated \\UXXXXXXXX escape (2181119185.py, line 75)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 75\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"\"\"\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 1775-1776: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "V1\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# Custom function used in the original model\n",
    "def grayscale_to_rgb(x):\n",
    "    return tf.image.grayscale_to_rgb(x)\n",
    "\n",
    "# Load the existing model\n",
    "custom_objects = {'grayscale_to_rgb': grayscale_to_rgb}\n",
    "model = load_model(\"NEW_colorization_model_epoch_1450.h5\", custom_objects=custom_objects)\n",
    "\n",
    "# Data preparation function\n",
    "def load_and_preprocess_image(grayscale_path, colorful_path, target_size=(256, 256)):\n",
    "    grayscale_img = tf.io.read_file(grayscale_path)\n",
    "    grayscale_img = tf.image.decode_png(grayscale_img, channels=1)\n",
    "    grayscale_img = tf.image.resize(grayscale_img, target_size)\n",
    "    grayscale_img = grayscale_img / 255.0\n",
    "\n",
    "    colorful_img = tf.io.read_file(colorful_path)\n",
    "    colorful_img = tf.image.decode_png(colorful_img, channels=3)\n",
    "    colorful_img = tf.image.resize(colorful_img, target_size)\n",
    "    colorful_img = colorful_img / 255.0\n",
    "\n",
    "    return grayscale_img, colorful_img\n",
    "\n",
    "# Create a tf.data.Dataset\n",
    "def create_dataset(grayscale_dir, colorful_dir, batch_size=16):\n",
    "    grayscale_imgs = sorted([os.path.join(grayscale_dir, fname) for fname in os.listdir(grayscale_dir) if fname.endswith('.png')])\n",
    "    colorful_imgs = sorted([os.path.join(colorful_dir, fname) for fname in os.listdir(colorful_dir) if fname.endswith('.png')])\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((grayscale_imgs, colorful_imgs))\n",
    "    dataset = dataset.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "# Prepare your data\n",
    "grayscale_dir = r\"C:\\Users\\dhana\\Downloads\\SAR Orig\\v_2\\urban\\s1\"\n",
    "colorful_dir = r\"C:\\Users\\dhana\\Downloads\\SAR Orig\\v_2\\urban\\s2\"\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = create_dataset(grayscale_dir, colorful_dir, batch_size)\n",
    "\n",
    "# Define callbacks\n",
    "checkpoint = ModelCheckpoint(\"improved_colorization_model.keras\", save_best_only=True, monitor='loss')\n",
    "early_stopping = EarlyStopping(patience=10, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(factor=0.2, patience=5, min_lr=1e-6)\n",
    "\n",
    "# Compile the model with a lower learning rate\n",
    "model.compile(optimizer=Adam(1e-4), loss='mse')\n",
    "\n",
    "# Fine-tune the model\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    callbacks=[checkpoint, early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"final_improved_colorization_model.keras\")\n",
    "\n",
    "print(\"Fine-tuning completed. Model saved as 'final_improved_colorization_model.keras'\")\n",
    "\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\dhana\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, array_to_img\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def grayscale_to_rgb(x):\n",
    "    return tf.image.grayscale_to_rgb(x)\n",
    "\n",
    "def preprocess_image(image_path, target_size=(256, 256)):\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    image = image.resize(target_size)  # Resize to the target size\n",
    "    image_array = img_to_array(image)  # Convert to numpy array\n",
    "    image_array = np.expand_dims(image_array, axis=0)  # Add batch dimension\n",
    "    image_array = image_array / 255.0  # Normalize to [0, 1]\n",
    "    return image_array\n",
    "\n",
    "def postprocess_image(image_array):\n",
    "    image_array = image_array.squeeze()  # Remove batch dimension\n",
    "    image_array = np.clip(image_array, 0, 1)  # Clip values to [0, 1]\n",
    "    image_array = (image_array * 255).astype(np.uint8)  # Convert to [0, 255] range\n",
    "    return array_to_img(image_array)  # Convert to PIL Image\n",
    "\n",
    "def test_colorization_model(model, input_image_path, output_image_path):\n",
    "    try:\n",
    "        original_image = Image.open(input_image_path).convert('L')\n",
    "        preprocessed_image = preprocess_image(input_image_path)\n",
    "        colorized_image_array = model.predict(preprocessed_image)\n",
    "        colorized_image = postprocess_image(colorized_image_array[0])\n",
    "        colorized_image.save(output_image_path)\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.imshow(original_image, cmap='gray')\n",
    "        plt.title('Original Grayscale')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.imshow(colorized_image)\n",
    "        plt.title('Colorized')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(f\"Colorized image saved to {output_image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "def test_on_input(model, input_path, output_path):\n",
    "    if os.path.isfile(input_path):\n",
    "        test_colorization_model(model, input_path, output_path)\n",
    "    elif os.path.isdir(input_path):\n",
    "        if not os.path.exists(output_path):\n",
    "            os.makedirs(output_path)\n",
    "        for filename in os.listdir(input_path):\n",
    "            if filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                input_file = os.path.join(input_path, filename)\n",
    "                output_file = os.path.join(output_path, f'colorized_{filename}')\n",
    "                test_colorization_model(model, input_file, output_file)\n",
    "    else:\n",
    "        print(f\"Invalid input path: {input_path}\")\n",
    "\n",
    "# Load the model\n",
    "custom_objects = {'grayscale_to_rgb': grayscale_to_rgb}\n",
    "generator = tf.keras.models.load_model(\"BestNEW_colorization_model.h5\", custom_objects=custom_objects)\n",
    "\n",
    "# Usage\n",
    "input_path = r\"C:\\Users\\dhana\\Downloads\\SAR Orig\\v_2\\urban\\s1\\ROIs1970_fall_s1_8_p36.png\"\n",
    "output_path = r\"C:\\Users\\dhana\\AppData\\Local\\Programs\\Python\\Python39\\op2.png\"\n",
    "test_on_input(generator, input_path, output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
